[
    {
        "content": "<p>Reading about the two-stage lookup tables for Unicode properties, and the implementation notes suggest the following is an optimal method.</p>\n<p>I've been scratching my head thinking about how to do this in Roc, and I'm not quite sure I really understand how this works. </p>\n<p>My understanding is that instead of just having a list that is <code>Num.MaxU32</code> in length and then doing a <code>getProperty = \\u32 -&gt; List.get u32</code>, you could break this into two tables which are smaller, and then do a lookup on the first table to get the offset into the second table. </p>\n<p>I'm probably getting tired, but how does this work? Is anyone able to provide a simple example of this?</p>\n<p>I'm reasonably confident I can generate the tables using a script, and I feel like I could sample the desired values by looping through all possible values, but I think I'm missing a key ingredient here.</p>\n<div class=\"codehilite\" data-code-language=\"C\"><pre><span></span><code><span class=\"kt\">unsigned</span><span class=\"w\"> </span><span class=\"nf\">get_property</span><span class=\"p\">(</span><span class=\"kt\">unsigned</span><span class=\"w\"> </span><span class=\"n\">ch</span><span class=\"p\">)</span>\n<span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"k\">const</span><span class=\"w\"> </span><span class=\"kt\">unsigned</span><span class=\"w\"> </span><span class=\"n\">BLOCK_SIZE</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mi\">256</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"kt\">unsigned</span><span class=\"w\"> </span><span class=\"n\">block_offset</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">stage1</span><span class=\"p\">[</span><span class=\"n\">ch</span><span class=\"w\"> </span><span class=\"o\">/</span><span class=\"w\"> </span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"w\"> </span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"k\">return</span><span class=\"w\"> </span><span class=\"n\">stage2</span><span class=\"p\">[</span><span class=\"n\">block_offset</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"w\"> </span><span class=\"n\">ch</span><span class=\"w\"> </span><span class=\"o\">%</span><span class=\"w\"> </span><span class=\"n\">BLOCK_SIZE</span><span class=\"p\">];</span>\n<span class=\"p\">}</span>\n</code></pre></div>",
        "id": 398876202,
        "sender_full_name": "Luke Boswell",
        "timestamp": 1698404751
    },
    {
        "content": "<p>IIRC a key detail here is that some (most?) of the blocks will actually be identical, and thus can be duplicated. That's the primary advantage of this method over just doing the large table look up.</p>",
        "id": 399001076,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1698461782
    },
    {
        "content": "<p>I saw a blog post a while ago about someone building a better compression method for these tables. I couldn't find it tho.</p>",
        "id": 399001149,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1698461836
    }
]