[
    {
        "content": "<p>Are all of those IRs separate data structures? Many (especially in the build phase) Feel like they should be the same IR just with multiple passes running on it.</p>",
        "id": 497212139,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738457044
    },
    {
        "content": "<p>Also, I feel like adding in reference counting is likely too late in the pipeline. The interpreter will need to run reference counting. So likely we will want refcounting right after check phase and to the interpreter run on the refcounted IR, but maybe I am missing something with the dependencies here.</p>",
        "id": 497212281,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738457219
    },
    {
        "content": "<p>I also definitely think we should think more about the build phase pass ordering. What would lead to the simplest implementation.</p>",
        "id": 497212380,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738457318
    },
    {
        "content": "<p>I think the interpreter would do its own runtime reference counting</p>",
        "id": 497212647,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738457600
    },
    {
        "content": "<p>separate system</p>",
        "id": 497212651,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738457605
    },
    {
        "content": "<p>Sure, but it still needs to know when to check reference counts, right? Like it needs to know when calling a function that the function requires incrementing the reference count of the 3rd arg passed in.</p>",
        "id": 497212717,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738457662
    },
    {
        "content": "<p>Though I guess that probably can still be figured out at runtime. Hmm.</p>",
        "id": 497212747,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738457706
    },
    {
        "content": "<p>That stage has a big question mark around it. Sam mentioned it's something he was really unsure about what it does or where it should go.</p>",
        "id": 497212764,
        "sender_full_name": "Luke Boswell",
        "timestamp": 1738457737
    },
    {
        "content": "<p>I see it as a few different pieces:</p>\n<ol>\n<li>You have inserting refcount increments and decrements based on lifetimes</li>\n<li>You have optimizations like drop specialization to remove a ton of refcounting changes by realizing when records or containers go out of scope but some of their elements stick around (like record destructuring).</li>\n<li>You have morphic which we don't plan to port and don't need to worry about for now, but can statically recognize uniqueness for mutation purposes.</li>\n<li>You have borrow inference which is super important for hot loop perf and allows avoiding refcounts if a function argument is only ever read from and never potentially mutated (currently only works for lists, but theoretically would be more powerful if it understands aggregates).</li>\n<li>Finally, you have actually generation of code that knows how to perform a refcount. For example, you have to know how to recursively decrement refcounts of complex datastructures. This makes code gen a lot simpler cause code gen no longer has to understand how to refcount types.</li>\n</ol>",
        "id": 497213039,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738458026
    },
    {
        "content": "<p>also, I'm definitely keen to help port and work on a lot of this backend stuff and our new wiring to llvm. I feel like I have a solid bar on many things after specialization (Though I have not actually looked into the algorithm for drop specialization).</p>\n<p>The nice thing with most of theses passes is that they are optional. So In the list above, you would start by porting just 1 and 5. Later 2, 3, and 4 can be added in.</p>",
        "id": 497213185,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738458219
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"343810\">Brendan Hansknecht</span> <a href=\"#narrow/channel/316715-contributing/topic/contributor.20coordination.20meeting.20-.20Feb.202024.20.232/near/497212747\">said</a>:</p>\n<blockquote>\n<p>Though I guess that probably can still be figured out at runtime. Hmm.</p>\n</blockquote>\n<p>yeah, for example Python does automatic reference counting fully at runtime</p>",
        "id": 497216398,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738461543
    },
    {
        "content": "<p>As a note, I realized I missed reset reuse which is also refcounting related. Lots of optimizations passes to eventually add but that I don't think are fundamentally needed.</p>\n<p>Oh, as another note talking of irs. Sharing irs between multiple passes means you can turn off passes or implement new passes without changing API boundaries significantly.</p>",
        "id": 497236291,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738481635
    },
    {
        "content": "<p>You're right. However, it's easier to ensure correctness when we have the shape of the IRs constrained to what each phase can actually use and actually produce. Less of a need to say \"I shouldn't have a function, save a compiler problem to a big list.\"</p>",
        "id": 497242823,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738488304
    },
    {
        "content": "<p>It's prioritizing correctness over performance</p>",
        "id": 497242843,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738488340
    },
    {
        "content": "<p>It really won't be that hard to change later</p>",
        "id": 497242855,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738488354
    },
    {
        "content": "<p>I think adding 10 to 20 clones of the ir for all the phases is still a mistake either way. And I don't think it helps correctness in many cases</p>",
        "id": 497271896,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738514850
    },
    {
        "content": "<p>As a first note, any pass that is optional, if it can avoid changing to a new IR, that really helps test correctness. It makes it easy to toggle any off, which is huge for debugging correctness.</p>",
        "id": 497272032,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738514986
    },
    {
        "content": "<p>I'm pretty sure <span class=\"user-mention\" data-user-id=\"453336\">@Joshua Warner</span> was running into multiple \"this should have been desugared\" errors while getting fuzzing up to speed</p>",
        "id": 497272069,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515003
    },
    {
        "content": "<p>We can assume testing will catch these things, but I'd always rather trust types than tests</p>",
        "id": 497272128,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515043
    },
    {
        "content": "<p>On top of that some passes like lifting lambdas really should not change the ir. It will be easier to move a few nodes around then make an entire new ir.</p>",
        "id": 497272132,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515050
    },
    {
        "content": "<p>And Ayaz keeps iterating that IR translation costs basically nothing compared to algorithmic costs like specialization</p>",
        "id": 497272158,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515074
    },
    {
        "content": "<p>Sure, but even ignoring the compile time cost, I think it hurts correctness for many phases (especially optimization phases that are optional)</p>",
        "id": 497272194,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515121
    },
    {
        "content": "<p>I don't understand how correctness could be hurt</p>",
        "id": 497272250,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515147
    },
    {
        "content": "<p>Cause you lose the ability to turn off a pass. It is now always required due to creating a new IR.</p>",
        "id": 497272266,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515169
    },
    {
        "content": "<p>I don't think any of our passes besides maybe refcounting and maaaaybeee lower_ir could be turned off</p>",
        "id": 497272345,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515227
    },
    {
        "content": "<p>reset reuse, drop specialization, borrow inference, morphic (if we ever re-add it), any other future optimization</p>",
        "id": 497272431,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515277
    },
    {
        "content": "<p>Oh, those, sure</p>",
        "id": 497272440,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515288
    },
    {
        "content": "<p>Those aren't in the preliminary design</p>",
        "id": 497272447,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515295
    },
    {
        "content": "<p>The 9 in the preliminary design are all required</p>",
        "id": 497272466,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515305
    },
    {
        "content": "<p>Everything past that could get away with reuse</p>",
        "id": 497272485,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515318
    },
    {
        "content": "<p>Ah, ok, then quite possibly it is fine.</p>",
        "id": 497272513,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515338
    },
    {
        "content": "<p>What are the difference in the middle IRs? like lift functions?</p>",
        "id": 497272571,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515363
    },
    {
        "content": "<p>The 9 I'm thinking about are:</p>\n<ul>\n<li>parse</li>\n<li>canonicalize (now has type ids, interned values)</li>\n<li>resolve imports (reused for typechecking) (any \"delayed import\" values are resolved to the actual values)</li>\n<li>specialize types (no more generic types anywhere, so normal type vars are gone)</li>\n<li>lift functions (lambdas should no longer exist in the code)</li>\n<li>solve functions (all higher-order function arguments need to be generic)</li>\n<li>specialize functions (no more higher-order functions anywhere)</li>\n<li>lower IR (converted to statements, refcounting shouldn't exist yet)</li>\n<li>refcount (refcounts added)</li>\n</ul>",
        "id": 497272581,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515372
    },
    {
        "content": "<p>I'll edit that message to talk about structural differences</p>",
        "id": 497272599,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515393
    },
    {
        "content": "<p>yes, please do, I would like to understand</p>",
        "id": 497272748,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515530
    },
    {
        "content": "<p>done</p>",
        "id": 497272749,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515532
    },
    {
        "content": "<p>Very terse descriptions, let me know what needs explaining</p>",
        "id": 497272764,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515546
    },
    {
        "content": "<p>The only one we might consider is solve functions, but that's playing with fire a little bit</p>",
        "id": 497272792,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515587
    },
    {
        "content": "<blockquote>\n<p>resolve imports (reused for typechecking) (any \"delayed import\" values are resolved to the actual values)</p>\n</blockquote>\n<p>Why is this an entire new IR and not just making a new dictionary of resolved imports or editting all import nodes and then banning the old type of import node?  What does a new IR help with?</p>",
        "id": 497272886,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515635
    },
    {
        "content": "<p>just one example</p>",
        "id": 497272900,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515646
    },
    {
        "content": "<p>As a relative percentage, like 90%+ of the IR is shared</p>",
        "id": 497273149,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738515866
    },
    {
        "content": "<p>yeah, so why make a new IR instead of just making a verifier that the old node type is gone/banned. That is exceptionally easy to fuzz/test correct</p>",
        "id": 497273184,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738515901
    },
    {
        "content": "<p>Yeah, some passes seem to just return subsets of the previous IRs instead of changing the fundamental shape of them</p>",
        "id": 497273605,
        "sender_full_name": "Agus Zubiaga",
        "timestamp": 1738516237
    },
    {
        "content": "<p><span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span> it's a question of trust. Not that big a deal either way</p>",
        "id": 497273788,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738516439
    },
    {
        "content": "<p>For context, I am not against spinning up new IRs, I have worked with mlir professionally, and it is made to make making new IRs super easy. In practice, making too many IRs always sucks. Verifiers and progressively adding more details generally works a lot better if IRs are similar.</p>",
        "id": 497273836,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738516456
    },
    {
        "content": "<blockquote>\n<p>it's a question of trust</p>\n</blockquote>\n<p>Verifiers can remove the need for trust.</p>",
        "id": 497273857,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738516487
    },
    {
        "content": "<p>The feedback on verifiers is far from instantaneous tho</p>",
        "id": 497273916,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738516548
    },
    {
        "content": "<p>e.g. you have to find a failing input first, which is not always obvious</p>",
        "id": 497273926,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738516558
    },
    {
        "content": "<p>If you have different IR types, the compiler will yell at you immediately</p>",
        "id": 497273976,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738516570
    },
    {
        "content": "<p>I agree that it gives some compile time guarantees around typing which can be nice.</p>",
        "id": 497274008,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738516627
    },
    {
        "content": "<p>I still am not sold it is worth 9 IRs, many which are exceptionally similar. As I mentioned above, from working with MLIR, too many dialects ends up being more painful to work with. I think there is a balance where some passes should share, but others should be split. Depends on the amount of difference between IRs.</p>",
        "id": 497274102,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738516713
    },
    {
        "content": "<p>Anyway, not a hill I will die on, I think 9 execeptionally similar IRs will just make a lot of extra work and maintenance burden for us with little gains.</p>",
        "id": 497274402,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738516949
    },
    {
        "content": "<p>I wonder if there are ways we can catch these at compile time without the entire new IR. Not sure.</p>",
        "id": 497274459,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738517019
    },
    {
        "content": "<p>Maybe storing the parts that are different out of band?</p>",
        "id": 497274558,
        "sender_full_name": "Agus Zubiaga",
        "timestamp": 1738517100
    },
    {
        "content": "<p>Not sure if that helps correctness, though, we would have to look at the specifics</p>",
        "id": 497274577,
        "sender_full_name": "Agus Zubiaga",
        "timestamp": 1738517118
    },
    {
        "content": "<p>Ah yeah, that would work. Then cause the old out of ban thing is no longer around, you are guaranteed to have transitioned.</p>",
        "id": 497274588,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738517137
    },
    {
        "content": "<p>Yeah, exactly</p>",
        "id": 497274643,
        "sender_full_name": "Agus Zubiaga",
        "timestamp": 1738517165
    },
    {
        "content": "<p>Though I guess you could still have a stale reference to the old out of ban data potentially.</p>",
        "id": 497274654,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738517176
    },
    {
        "content": "<p>There could be debug-mode checks for that I think</p>",
        "id": 497274697,
        "sender_full_name": "Agus Zubiaga",
        "timestamp": 1738517225
    },
    {
        "content": "<p>Yeah, just like verifiers on the same IR for classes of nodes being gone.</p>",
        "id": 497275228,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738517681
    },
    {
        "content": "<p>Also, one simple example of maintenance pain. If you add a new IR node, you now how to add it to an wire it through 9 dialects instead of 3. Any larger refactor also becomes multiple times bigger if you have more IRs.</p>",
        "id": 497275669,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518053
    },
    {
        "content": "<p>You lose the ability to ever reorder passes without significant hassle (may not matter for roc if we truly know the pipeline, but has been a big pain in practices for work I have done before)</p>",
        "id": 497275741,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518118
    },
    {
        "content": "<p>If passes have fundamentally different requirements and guarantees on input and output (e.g. it expects a specific node to have been desugared), then it's already not re-orderable</p>",
        "id": 497275850,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738518165
    },
    {
        "content": "<p>Some passes are not re-orderable. That is true. But that is not true of all passes</p>",
        "id": 497275892,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518208
    },
    {
        "content": "<p>And that is where verifiers come in.</p>",
        "id": 497275904,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518223
    },
    {
        "content": "<p>I would contend that it is true of all passes that should have separate IRs</p>",
        "id": 497275913,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738518233
    },
    {
        "content": "<p>(and visa-versa)</p>",
        "id": 497275970,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738518258
    },
    {
        "content": "<blockquote>\n<p>If you add a new IR node, you now how to add it to an wire it through 9 dialects instead of 3.</p>\n</blockquote>\n<p>This \"wire things around\" kind of update is the sort of thing I expect LLM-driven tooling to get much better at very quickly.</p>",
        "id": 497276058,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738518322
    },
    {
        "content": "<p>I think that is a bad metric for what is a maintainable and fun to work on compiler.</p>",
        "id": 497276204,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518425
    },
    {
        "content": "<p>Fair <span aria-label=\"shrug\" class=\"emoji emoji-1f937\" role=\"img\" title=\"shrug\">:shrug:</span></p>",
        "id": 497276219,
        "sender_full_name": "Joshua Warner",
        "timestamp": 1738518439
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"461444\">@Sam Mohr</span> is specialize where we make N copies of each function?</p>",
        "id": 497276328,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518510
    },
    {
        "content": "<p>If so, we may want to order passes as:</p>\n<ol>\n<li>lower IR</li>\n<li>Add in refcounting and do any sort of refcounting optimizations (not sure we can do all the optimizations before specialization)</li>\n<li>specialize</li>\n<li>code gen concrete refcounting functions (this will also turn unused refcount nodes into noops)</li>\n<li>gen llvm/other backends</li>\n</ol>\n<p>Note: for 2, I think at a minimum we can do the refcount insertion and borrow inference. May not be able to do other optimization like reset reuse.</p>",
        "id": 497276687,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738518834
    },
    {
        "content": "<p>How can we do refcounting for a function that uses either a list or an int? We'd have to optionally do it</p>",
        "id": 497299961,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537241
    },
    {
        "content": "<p>If you're doing this level of reordering, we need a expert to agree on it, aka <span class=\"user-mention\" data-user-id=\"454654\">@Ayaz Hafiz</span> . I'm pretty sure we need to make everything first order before we start lowering</p>",
        "id": 497300015,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537298
    },
    {
        "content": "<p>Refcounting is the same. Int refcounting will just turn to a noop when finalized. So we would insert refcounts for everything concrete that needs refcounting and anything dynamic that might need refcounting.</p>",
        "id": 497300694,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738537810
    },
    {
        "content": "<p>Why not just do refcounting once at the end?</p>",
        "id": 497300750,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537861
    },
    {
        "content": "<p>I see your goal</p>",
        "id": 497300765,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537880
    },
    {
        "content": "<p>Which is to specialize later to avoid doing the same work we need to do later on more copies</p>",
        "id": 497300790,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537899
    },
    {
        "content": "<p>Just trying to move as much stuff as possible before specialization to avoid repeating it n times</p>",
        "id": 497300793,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738537902
    },
    {
        "content": "<p>Yeah, exactly</p>",
        "id": 497300800,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738537911
    },
    {
        "content": "<p>I'm just assuming <span class=\"user-mention\" data-user-id=\"454654\">@Ayaz Hafiz</span> would have suggested that was an option if we could</p>",
        "id": 497300860,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537930
    },
    {
        "content": "<p>But maybe I'm putting words in his mouth</p>",
        "id": 497300867,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738537939
    },
    {
        "content": "<p>Also, I assume specializing a dense ir will be faster than specializing a recursive ir. Thus moving lower ir before specialization. I think for the most part this shouldn't hurt complexity/correctness much. Just a different order.</p>",
        "id": 497300910,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738537986
    },
    {
        "content": "<p>But fundamentally the same work</p>",
        "id": 497300917,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738537992
    },
    {
        "content": "<p>Actually, I don't know how lowering to statements would work with child functions not having been lifted yet</p>",
        "id": 497300955,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538036
    },
    {
        "content": "<p>I'm not sure I follow isn't lifting functions much earlier in the pipeline?</p>",
        "id": 497301011,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538062
    },
    {
        "content": "<p>Can we spin these off into separate discussion threads.. and keep the contributor coord meeting thread scoped to planning/scheduling discussion. It's going to be hard to find things later.</p>",
        "id": 497301021,
        "sender_full_name": "Luke Boswell",
        "timestamp": 1738538072
    },
    {
        "content": "<p>It's after type specialization</p>",
        "id": 497301022,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538074
    },
    {
        "content": "<p>I'm on mobile, could someone else do it?</p>",
        "id": 497301037,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538091
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"461444\">@Sam Mohr</span> can you explain the difference betteen type and function specialization? I assumed function specialization is what leads to N copies of each function, but maybe that is wrong?</p>",
        "id": 497301229,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538258
    },
    {
        "content": "<p>No, it's different</p>",
        "id": 497301305,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538299
    },
    {
        "content": "<p>Type specialization takes a copy of every top level value and every top level function and makes a concretely-typed copy for each specific set of types it should have at runtime</p>",
        "id": 497301344,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538347
    },
    {
        "content": "<p>Function specialization is <a href=\"https://github.com/roc-lang/rfcs/blob/ayaz/compile-with-lambda-sets/0102-compiling-lambda-sets.md#function_specialize\">https://github.com/roc-lang/rfcs/blob/ayaz/compile-with-lambda-sets/0102-compiling-lambda-sets.md#function_specialize</a></p>",
        "id": 497301450,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538430
    },
    {
        "content": "<p>oh....</p>",
        "id": 497301452,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538437
    },
    {
        "content": "<p>Which is a way to convert higher order functions to functions we call using a tag union to represent different possible arguments</p>",
        "id": 497301480,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538468
    },
    {
        "content": "<p>It's kinda like, if not exactly, defunctionalization</p>",
        "id": 497301491,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538480
    },
    {
        "content": "<p>Confusing name, I agree</p>",
        "id": 497301517,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538508
    },
    {
        "content": "<p>and type specialization has to come before all of these:</p>\n<blockquote>\n<ul>\n<li>lift functions (lambdas should no longer exist in the code)</li>\n<li>solve functions (all higher-order function arguments need to be generic)</li>\n<li>specialize functions (no more higher-order functions anywhere)</li>\n</ul>\n</blockquote>",
        "id": 497301537,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538522
    },
    {
        "content": "<p>We could maybe call it defunctionalize to make the name distinct</p>",
        "id": 497301577,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538525
    },
    {
        "content": "<p>It doesn't have to, but it's gonna be pretty hard to do those correctly otherwise I think</p>",
        "id": 497301600,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538559
    },
    {
        "content": "<p>ack</p>",
        "id": 497301605,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538565
    },
    {
        "content": "<p>That section of the compiler, even if we combine those phases, will be hard to reorder</p>",
        "id": 497301616,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538578
    },
    {
        "content": "<p>Yeah, then suggested order sounds fine</p>",
        "id": 497301640,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538600
    },
    {
        "content": "<p>Though it'd be nice to avoid doing more work for the phases after them because of the type specialization</p>",
        "id": 497301650,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538613
    },
    {
        "content": "<p>Yeah, would be nice if we could share more. Cause, for example, if you have 10 specializations of the same function that uses lists, they all refcount the same way.</p>",
        "id": 497301816,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538755
    },
    {
        "content": "<p>But unless that is easy to do, I 100% agree with correctness first.</p>",
        "id": 497301893,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738538791
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"343810\">Brendan Hansknecht</span> <a href=\"#narrow/stream/395097-compiler-development/topic/zig.20compiler.20-.20IRs.20and.20pass.20ordering/near/497275669\">said</a>:</p>\n<blockquote>\n<p>Also, one simple example of maintenance pain. If you add a new IR node, you now how to add it to an wire it through 9 dialects instead of 3. Any larger refactor also becomes multiple times bigger if you have more IRs.</p>\n</blockquote>\n<p>Not sure if we'll come to an agreement, but it feels like we're not on the same page, so I'll ask: if we can't really reorder from the suggested order, then are you still against the IRs being different?</p>\n<p>One big benefit of different IRs is that if we make sure that calculating stage N takes the stage N+1 IRs for the dep modules, then we have a type guarantee that the deps must have been compiled first</p>",
        "id": 497302069,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538923
    },
    {
        "content": "<p>I think it's my lizard brain speaking, but I see fuzzing as a high probability test for correctness, and types as true guarantees</p>",
        "id": 497302119,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738538970
    },
    {
        "content": "<p>I always prefer types where possible, but I'm definitely always a high-level thinker for modeling problems</p>",
        "id": 497302178,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1738539002
    },
    {
        "content": "<p>my heuristic for this would be:</p>\n<ul>\n<li>material change in the invariants of the program: use a different IR (parsed/can/type checked/specialized/function specialized/etc)</li>\n<li>optimizations or simple invariant: use same IR</li>\n</ul>",
        "id": 497302263,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738539061
    },
    {
        "content": "<p>all optimizations over the so called mono IR, refcounting etc are probably same IR</p>",
        "id": 497302289,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738539085
    },
    {
        "content": "<p>cost of creating a new IR is a bad reason not to create an IR IMO</p>",
        "id": 497302303,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738539099
    },
    {
        "content": "<p>the IR creation being redundant and obfuscating the actual transformation is a good reason not to create an IR, which is the case for refcounting and optimizations</p>",
        "id": 497302382,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738539143
    },
    {
        "content": "<p>Yeah, that sounds pretty reasonable overall as a metric. It's up to the stage author to decide if invariants are simple enough to reuse the same IR. In my mind, many of the \"its the same IR minus one specific thing\" are pretty simple invariants, but I won't be writing those passes, so whoever writes the passes can decide.</p>",
        "id": 497302806,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738539476
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"454654\">@Ayaz Hafiz</span> do you think any work can be hoisted above type specialization to avoid repeating N times? Just curious if there is anything that is flexible that we should try to move now instead of later when the pipeline is more ironed down.</p>",
        "id": 497302893,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738539546
    },
    {
        "content": "<p>what is the worry about what might be repeated N times?</p>",
        "id": 497303782,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540162
    },
    {
        "content": "<p>It is N times faster to do work once than N times. So anything that is reasonable to move before specialization probably should be.</p>",
        "id": 497303943,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738540294
    },
    {
        "content": "<p>i see</p>",
        "id": 497303968,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540318
    },
    {
        "content": "<p>i probably wouldn't think about this too deeply right now</p>",
        "id": 497304015,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540331
    },
    {
        "content": "<p>I don't want to think too deeply about it, but if there is anything we think would be easy to move now without a concern for correctness, I think we should move it. I think later it will be much harder to fix these kinds of issues.</p>",
        "id": 497304051,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1738540374
    },
    {
        "content": "<p>if there's something obvious that can be lifted it to an earlier pass it will be obviously and be easy to lift i think</p>",
        "id": 497304059,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540382
    },
    {
        "content": "<p>for example function lifting can kind of be done whenever after typechecking</p>",
        "id": 497304090,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540417
    },
    {
        "content": "<p>but that is one of those kind of obvious/trivial things</p>",
        "id": 497304102,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540427
    },
    {
        "content": "<p>i think its worth embracing that all of this will have to be rewritten at the limit anyway</p>",
        "id": 497304150,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540444
    },
    {
        "content": "<p>if you want the fastest compiler there are some parts of this architecture that will be blockers</p>",
        "id": 497304167,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738540459
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"454654\">Ayaz Hafiz</span> <a href=\"#narrow/channel/395097-compiler-development/topic/zig.20compiler.20-.20IRs.20and.20pass.20ordering/near/497304150\">said</a>:</p>\n<blockquote>\n<p>i think its worth embracing that all of this will have to be rewritten at the limit anyway</p>\n</blockquote>\n<p>I don't want to embrace that</p>",
        "id": 497307015,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542424
    },
    {
        "content": "<p>I think that will lead us to an unnecessarily and noticeably slow compiler for years and years</p>",
        "id": 497307035,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542439
    },
    {
        "content": "<p>I think that's what most compilers do, and I think we can do better without going overboard like we did before</p>",
        "id": 497307063,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542464
    },
    {
        "content": "<p>I do think it's fine to have different IRs for each pass (when it would be valuble)</p>",
        "id": 497307115,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542486
    },
    {
        "content": "<p>but I think eventually once things have settled down we might explore combining some of them as an incremental change</p>",
        "id": 497307138,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542514
    },
    {
        "content": "<p>eventually as in \"someday\" not like right after 0.1.0 or anything <span aria-label=\"sweat smile\" class=\"emoji emoji-1f605\" role=\"img\" title=\"sweat smile\">:sweat_smile:</span></p>",
        "id": 497307148,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542528
    },
    {
        "content": "<p>and I think that can be done more incrementally than rewriting the whole compiler</p>",
        "id": 497307156,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542541
    },
    {
        "content": "<p>i don't know. i don't think you can have both worlds. the current implementation is closer to what the limit needs to look like. but there's a reasoning a rewrite is being discussed.</p>",
        "id": 497307173,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738542552
    },
    {
        "content": "<p>I'm saying I think we should prioritize correctness, simplicity, maintainability, and debugabillity for this rewrite, but I think \"embrace that it's all gonna be rewritten and don't worry about it\" goes too far</p>",
        "id": 497307282,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542614
    },
    {
        "content": "<p>im not saying this approach will be too slow or anything like that. it will be totally fine for the vast majority of programs because limiting factors for any non-trivial program are going to be algorithmic, not number of passes or whatever.</p>",
        "id": 497307288,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738542621
    },
    {
        "content": "<p>well, i'm just saying that the fastest possible approach is not this approach</p>",
        "id": 497307315,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738542645
    },
    {
        "content": "<p>for sure!</p>",
        "id": 497307336,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542662
    },
    {
        "content": "<p>I just think we shouldn't be all-or-nothing about it haha</p>",
        "id": 497307351,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542672
    },
    {
        "content": "<p>before the goal was \"fastest possible\" and that got us into trouble</p>",
        "id": 497307367,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542688
    },
    {
        "content": "<p>I don't think we should aim for \"fastest possible\" for this rewrite</p>",
        "id": 497307379,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542697
    },
    {
        "content": "<p>but I also don't think we should aim for \"only algorithmic perf matters\" because I don't think that will turn out to be correct</p>",
        "id": 497307396,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542712
    },
    {
        "content": "<p>I mean maybe I'll be wrong!</p>",
        "id": 497307401,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542718
    },
    {
        "content": "<p>i think it's true. the only cases i can think of in the current implementation where it was a problem was because of algorithms. but maybe i'm missing cases you have in mind?</p>",
        "id": 497307471,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738542753
    },
    {
        "content": "<p>I agree with that, but \"the current thing that's heavily optimized for perf only has algorithmic trouble\" does not imply \"if we have one that's not optimized at all for perf it will also only have algorithmic trouble\"</p>",
        "id": 497307532,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542814
    },
    {
        "content": "<p>that would imply that only algorithmic performance optimizations matter to compilers as a category, which I would be extremely skeptical of haha</p>",
        "id": 497307594,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738542844
    },
    {
        "content": "<p>i agree</p>",
        "id": 497307627,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738542880
    },
    {
        "content": "<p>i think the difference is in limiting factors. it's far more likely that algorithmic passes are the limiting factor because Roc is doing a lot work to lower a high level language to a low level one, than just rewriting stuff to a different form. But at the limit you have to fuse both, and this approach doesn't really work for that, I think.</p>",
        "id": 497307759,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738542976
    },
    {
        "content": "<p>but also like i think they should be two different planes</p>",
        "id": 497307814,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543031
    },
    {
        "content": "<p>the concerns about the algorithmic passes should be entirely disjoint from the implementation of how data is stored</p>",
        "id": 497307832,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543054
    },
    {
        "content": "<p>I guess a really big question mark here is whether the \"dev backend can be an interpreter and it's fine\" hypothesis proves correct in practice</p>",
        "id": 497307842,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543066
    },
    {
        "content": "<p>that way you have a nice interface boundary between the two and can prefer to optimize one or the other</p>",
        "id": 497307851,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543073
    },
    {
        "content": "<p>because if that works out, then in dev builds we stop after type-checking</p>",
        "id": 497307887,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543083
    },
    {
        "content": "<p>and only <code>--optimize</code> builds do any of this expensive stuff</p>",
        "id": 497307900,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543092
    },
    {
        "content": "<p>if that kind of boundary can be done correctly, then you don't have to worry about getting both right at the same time</p>",
        "id": 497307910,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543108
    },
    {
        "content": "<p>which get run way less frequently, and the downside of a slower feedback loop is much less (unless you're doing performance optimization, in which case it's painful if they're slow)</p>",
        "id": 497307917,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543118
    },
    {
        "content": "<p>I think it will work out fine, taking e.g python or ocaml as prior</p>",
        "id": 497307926,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543126
    },
    {
        "content": "<p>OCaml is interpreted? <span aria-label=\"astonished\" class=\"emoji emoji-1f632\" role=\"img\" title=\"astonished\">:astonished:</span></p>",
        "id": 497307939,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543139
    },
    {
        "content": "<p>both of which don't have sophisticated interpreters (or at least didn't until the past few years)</p>",
        "id": 497307951,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543144
    },
    {
        "content": "<p>ocaml has many modes</p>",
        "id": 497307957,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543148
    },
    {
        "content": "<p>TIL!</p>",
        "id": 497307971,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543165
    },
    {
        "content": "<p>yeah the specific scenario I'm concerned about (because I don't have any idea how likely it is to come up in practice) is something like:</p>\n<ul>\n<li>I'm making a game in Roc</li>\n<li>If I run it in <code>--optimize</code> it plays great but my feedback loop is slow</li>\n<li>If I run it with the interpreter I have a fast feedback loop but the game is unplayably laggy so I have to pass <code>--optimize</code> to try anything out</li>\n<li>If I had a dev backend, the game would be fast enough to play and the feedback loop would be fast</li>\n</ul>",
        "id": 497308554,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543569
    },
    {
        "content": "<p>Brendan pointed out that our current dev backend is so unoptimized that it might not actually be faster enough than an interpreter to be noticeable</p>",
        "id": 497308573,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543598
    },
    {
        "content": "<p>and also, we can potentially JIT up the interpreter if <span aria-label=\"point up\" class=\"emoji emoji-1f446\" role=\"img\" title=\"point up\">:point_up:</span> becomes a problem in practice</p>",
        "id": 497308590,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543618
    },
    {
        "content": "<p>which also would still skip all the steps after type-checking for dev builds</p>",
        "id": 497308600,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543633
    },
    {
        "content": "<p>i think this is the kind of thing where it's probably just worth punting until it comes up</p>",
        "id": 497308708,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543696
    },
    {
        "content": "<p>if the implementation is simple, then it's not hard to refactor to make this work or introduce complexity for specific usage patterns</p>",
        "id": 497308732,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543718
    },
    {
        "content": "<p>and maybe it never comes up</p>",
        "id": 497308758,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543741
    },
    {
        "content": "<p>\"this\" being the IRs after type-checking?</p>",
        "id": 497308779,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1738543761
    },
    {
        "content": "<p>well just the compiler in general</p>",
        "id": 497308786,
        "sender_full_name": "Ayaz Hafiz",
        "timestamp": 1738543772
    },
    {
        "content": "<p>just a note I remembered: today, we have automatic dead code elimination of unused constants (e.g. I use a package but I only use some of its exposed operations, and nothing I actually use involves certain top-level constants the package defines) because we compile them to thunks which automatically get DCE'd</p>",
        "id": 501140202,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1740154743
    },
    {
        "content": "<p>but in the new compiler where we're instead evaluating constants at compile-time, we need to make sure that we don't include constants that have been compiled down to literals unless they're actually referenced somewhere!</p>",
        "id": 501140357,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1740154798
    },
    {
        "content": "<p>(same goes for string literals etc in general)</p>",
        "id": 501140445,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1740154811
    },
    {
        "content": "<p>Depends on what section granularity we use</p>",
        "id": 501149617,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1740157791
    },
    {
        "content": "<p>You can also put each constant in their own section just like functions.</p>",
        "id": 501149732,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1740157815
    },
    {
        "content": "<p>true, although it'll improve build speed if we just only flag things as used when we actually use them, and then only include them if so</p>",
        "id": 501150701,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1740158126
    },
    {
        "content": "<p>(compared to adding everything and then having the linker remove them again in a later pass)</p>",
        "id": 501150735,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1740158139
    },
    {
        "content": "<p>Same for functions preferably</p>",
        "id": 501155685,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1740159842
    },
    {
        "content": "<p>I think we could use specialization to do a liveness check even on functions that are already concrete.</p>",
        "id": 501155958,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1740159948
    },
    {
        "content": "<p>we've always done it for functions</p>",
        "id": 501156219,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1740160036
    },
    {
        "content": "<p>Oh, never realized</p>",
        "id": 501156661,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1740160186
    },
    {
        "content": "<p>I think we can have type specialization handle this</p>",
        "id": 501168043,
        "sender_full_name": "Sam Mohr",
        "timestamp": 1740164260
    }
]