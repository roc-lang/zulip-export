[
    {
        "content": "<p>this is interesting...something we could think about for our hashing API: <a href=\"https://purplesyringa.moe/blog/thoughts-on-rust-hashing/\">https://purplesyringa.moe/blog/thoughts-on-rust-hashing/</a></p>",
        "id": 488726134,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1734040757
    },
    {
        "content": "<p>Only starting to skim this, but the author is already pointing out many of my complaints with the hashing api</p>",
        "id": 488731091,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734042939
    },
    {
        "content": "<p>Though roc will not end up being accidentally quadradic (thanks index map)</p>",
        "id": 488731170,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734042965
    },
    {
        "content": "<blockquote>\n<p>How this API should look like and whether it can be shoehorned into the existing interfaces remains to be seen. I have not started work on the design yet, and perhaps this article might be a bit premature</p>\n</blockquote>\n<p>Oh, they don't really have any sort of suggestion or solution</p>",
        "id": 488731543,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043174
    },
    {
        "content": "<p>Also, this is why in roc, hasing a list of ints is super slow, but hashing an array of bytes is fast.</p>",
        "id": 488731640,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043207
    },
    {
        "content": "<p>We really need some sort of way to aggregate things. Especially all of our primitives types that could just be hashed as a giant chunk of bytes</p>",
        "id": 488731703,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043250
    },
    {
        "content": "<p>I tried at one point to do a streaming hasher (probably should try again), but the perf was pretty abysmal due to having to copy all bytes over to a separate list for accumulation. I think that probably could be done a lot smarter, but as mentioned in the artical, it is adhoc in an inconvenient way. This is especially true for fast hashers for hash tables</p>",
        "id": 488731871,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043327
    },
    {
        "content": "<p>For more robust and secure hashes, the copy matters a lot less</p>",
        "id": 488731887,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043338
    },
    {
        "content": "<p>All hash algorithms really just want to operate on a giant array of bytes. The more things that can be viewed that way, the better</p>",
        "id": 488731957,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043368
    },
    {
        "content": "<p>yeah, one vague idea I had was to maybe allow our derived hashing implementations to cheat and view things as raw bytes even though a handwritten pure roc implementation couldn't do that</p>",
        "id": 488732835,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1734043803
    },
    {
        "content": "<p>yeah, though you probably shouldn't hash padding (though I guess you could, but would make hashing system dependent) and you also can't do that with more complex types that contain pointers.</p>",
        "id": 488733005,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043902
    },
    {
        "content": "<p>Also, I guess this is their core idea:</p>\n<blockquote>\n<p>In my opinion, <code>Hasher</code> and <code>Hash</code> are a wrong abstraction. Instead of the <code>Hash</code> driving the <code>Hasher</code> <del>insane</del>, it should be the other way round: <code>Hash</code> providing introspection facilities and <code>Hasher</code> navigating the hashed objects recursively. As a bonus, this could enable (opt-in) portable hashers.</p>\n</blockquote>",
        "id": 488733120,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734043950
    },
    {
        "content": "<p>I feel like the idea here when mapped to roc would be to have the hasher use the hash spec to create a bespoke lambda that will hash in the correct block size. It will aggregate the input into blocks and would be smart enough to be able to aggregate many list elements into a single block instead of hashing each of them separate.</p>",
        "id": 488733569,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734044200
    },
    {
        "content": "<p>so instead of thinking from smallest unit to biggest, it would be using a spec to think about the whole picture at once.</p>",
        "id": 488733767,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734044313
    },
    {
        "content": "<p>Still not sure how it would work in practice, but worth messing around with the idea.</p>",
        "id": 488733802,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734044335
    },
    {
        "content": "<p>agreed!</p>",
        "id": 488733938,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1734044405
    },
    {
        "content": "<p>we'll have a perfect opportunity to do that with static dispatch</p>",
        "id": 488733992,
        "sender_full_name": "Richard Feldman",
        "timestamp": 1734044428
    },
    {
        "content": "<p>Also, adding small list optimization for list u8, probably would help a lot here. Then can just convert any numeric primitive straight to bytes without an allocation</p>",
        "id": 488734111,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734044505
    },
    {
        "content": "<p>my biggest concern is that the logic to come up with the correct hashing would be quite complex and expensive. I guess if the logic generates a lambda, it could be run once and store in a dict object for example.</p>",
        "id": 488734440,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734044701
    },
    {
        "content": "<p>I've been thinking about this more. I feel like for the general case for hashing a list of things, the best bet would be to hash it as an array of bytes, but have a bit of logic that when loading a chunk of the list will automatically zero out all padding bytes.</p>",
        "id": 489599606,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734483917
    },
    {
        "content": "<p>The two main pain points are:</p>\n<ol>\n<li>User defined hashes</li>\n<li>Types that are more padding than not</li>\n<li>Nested types.</li>\n</ol>\n<p>But I'm the general case, this probably would be way faster.</p>",
        "id": 489599722,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734484001
    },
    {
        "content": "<p>If user defined hashes was only allowed to decide extra bytes to consider padding (basically ignore some fields), it would also work in this form. But more complex user padding would still be problematic</p>",
        "id": 489599814,
        "sender_full_name": "Brendan Hansknecht",
        "timestamp": 1734484073
    }
]